{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Install Libraries"
      ],
      "metadata": {
        "id": "SKm74kG8cqj-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C4_vqXkzYnV1"
      },
      "outputs": [],
      "source": [
        "!pip install accelerate"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LLM setup and configuration"
      ],
      "metadata": {
        "id": "i8WSdK55c0Kc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "\n",
        "# Specify the LLM model we'll be using\n",
        "model_name = \"microsoft/Phi-3-mini-4k-instruct\"\n",
        "\n",
        "# Configure for GPU usage\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\", # Automatically use available GPU\n",
        "    torch_dtype=torch.float16, # Can improve performance on some GPUs\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "# Load the tokenizer for the chosen model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Create a pipeline object for easy text generation with the LLM\n",
        "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "15EFhje4ZbR8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configure LLM generation parameters"
      ],
      "metadata": {
        "id": "n-5zPi0-c4Ld"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Parameters to control LLM's response generation\n",
        "generation_args = {\n",
        "    \"max_new_tokens\": 512,     # Maximum length of the response\n",
        "    \"return_full_text\": False, # Only return the generated text\n",
        "}"
      ],
      "metadata": {
        "id": "YepqV76TZs6P"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Builing the `query` function"
      ],
      "metadata": {
        "id": "-6nP8Nwsc-X9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def query(messages):\n",
        "  \"\"\"\n",
        "  Sends a conversation history to the AI assistant and returns the answer.\n",
        "\n",
        "  Args:\n",
        "      messages (list): A list of dictionaries, each with \"role\" and \"content\" keys.\n",
        "\n",
        "  Returns:\n",
        "      str: The answer from the AI assistant.\n",
        "  \"\"\"\n",
        "\n",
        "  # Send the full conversation history to the LLM\n",
        "  output = pipe(messages, **generation_args)\n",
        "\n",
        "  # Extract the response\n",
        "  return output[0]['generated_text']"
      ],
      "metadata": {
        "id": "ESrgWI4fb7K5"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example usage of the `query` function"
      ],
      "metadata": {
        "id": "wor6eGYGdG2v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example: Math Problem\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful digital assistant. Please provide safe, ethical and accurate information to the user.\"},\n",
        "    {\"role\": \"user\", \"content\": \"What about solving the equation 2x + 3 = 7?\"}\n",
        "]\n",
        "result = query(messages)\n",
        "print(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FTQluA0bcPns",
        "outputId": "49b44049-3935-4bb1-a637-de73decfff49"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:transformers_modules.microsoft.Phi-3-mini-4k-instruct.29216027ca5f6555ed283a0606516fd72a614033.modeling_phi3:You are not running the flash-attention implementation, expect numerical differences.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "To solve the equation 2x + 3 = 7, follow these steps:\n",
            "\n",
            "1. Isolate the variable (x) by moving the constant term (3) to the other side of the equation. To do this, subtract 3 from both sides:\n",
            "\n",
            "   2x + 3 - 3 = 7 - 3\n",
            "\n",
            "   2x = 4\n",
            "\n",
            "2. Now, divide both sides of the equation by the coefficient of x (2) to solve for x:\n",
            "\n",
            "   2x / 2 = 4 / 2\n",
            "\n",
            "   x = 2\n",
            "\n",
            "So, the solution to the equation 2x + 3 = 7 is x = 2.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Builing the `chat` function"
      ],
      "metadata": {
        "id": "zOHXxpu8iMzJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def chat():\n",
        "  \"\"\"Enables interactive chat sessions with the AI assistant.\"\"\"\n",
        "\n",
        "  # Initialize the conversation with instructions for the AI assistant\n",
        "  conversation_history = [\n",
        "      {\"role\": \"system\", \"content\": \"You are a helpful digital assistant. Please provide safe, ethical and accurate information to the user.\"}\n",
        "  ]\n",
        "\n",
        "  # Main chat loop\n",
        "  while True:\n",
        "    user_input = input(\"You: \")\n",
        "\n",
        "    # Check if the user wants to exit the chat\n",
        "    if user_input.lower() == \"exit\":\n",
        "        break\n",
        "\n",
        "    # Add user's message to the conversation history\n",
        "    conversation_history.append({\"role\": \"user\", \"content\": user_input})\n",
        "\n",
        "    # Get a response from the AI assistant\n",
        "    try:\n",
        "        response = query(conversation_history)\n",
        "        print(\"\\nAssistant: \", response, \"\\n\")\n",
        "\n",
        "        # Record the AI assistant's response in the conversation history\n",
        "        conversation_history.append({\"role\": \"assistant\", \"content\": response})\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}, please try again.\")"
      ],
      "metadata": {
        "id": "DYuKx5uAcS7n"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initiating a chat session using the `chat` function"
      ],
      "metadata": {
        "id": "jtbwJqPZiICo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chat()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8X5YAXffski",
        "outputId": "20c2d70b-6d6c-47fa-9e0b-334c81822cc1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You: Hi\n",
            "\n",
            "Assistant:  Hello! How can I assist you today? Whether you have questions or need information, I'm here to help. \n",
            "\n",
            "You: Where is Ahmedabad located?\n",
            "\n",
            "Assistant:  Ahmedabad is a major city in the western Indian state of Gujarat. It is situated on the banks of the Sabarmati River and is the administrative headquarters of the Ahmedabad district. Ahmedabad is known for its rich history, vibrant culture, and significant contributions to India's textile industry. The city is also famous for its beautiful architecture, including the historic stepwells and the iconic Ambaji Temple. \n",
            "\n",
            "You: During my visit, what are the top 3 places I should go?\n",
            "\n",
            "Assistant:  During your visit to Ahmedabad, you should consider visiting the following top three places:\n",
            "\n",
            "\n",
            "1. **Sayaji Baug**: This is one of the largest botanical gardens in Asia, offering a serene escape with a variety of plants, flowers, and a beautiful lake. It's a great place for a leisurely walk and to enjoy the greenery.\n",
            "\n",
            "\n",
            "2. **Manek Chowk**: Known as the heart of Ahmedabad, Manek Chowk is a bustling marketplace with a rich history. It's a great place to experience the local culture, shop for textiles, and try out traditional Gujarati cuisine.\n",
            "\n",
            "\n",
            "3. **Sardar Patel Stadium**: For sports enthusiasts, this stadium is a must-visit. It's the home ground for the Gujarat cricket team and hosts various sports events. The stadium also offers a panoramic view of the city.\n",
            "\n",
            "\n",
            "These places offer a mix of cultural, historical, and recreational experiences that are quintessential to Ahmedabad. \n",
            "\n",
            "You: exit\n"
          ]
        }
      ]
    }
  ]
}